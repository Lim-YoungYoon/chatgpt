{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://openai.com/blog/introducing-text-and-code-embeddings  \n",
    "https://platform.openai.com/docs/guides/embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # .env 파일에서 환경 변수 로드\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')  # OPENAI_API_KEY 환경 변수 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np # 벡터 연산을 위해 numpy 라이브러리를 임포트합니다.\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_with(persona, ask_user):\n",
    "\n",
    "    # GPT-3 모델에 전달할 메시지 쿼리를 생성합니다.\n",
    "    query = [{\"role\": \"system\", \"content\": persona}, {\"role\": \"user\", \"content\": ask_user}]\n",
    "\n",
    "    # GPT-3 모델에 메시지 쿼리를 전달하여 응답을 받습니다.\n",
    "    result = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=query,\n",
    "        n=3\n",
    "    )\n",
    "\n",
    "    # GPT-3 모델의 응답 메시지를 반환합니다.\n",
    "    return [choice[\"message\"][\"content\"] for choice in result[\"choices\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query : Who do you think is the best soccer player and why\n",
      "document : I cannot definitively name the best soccer player as it is a subjective topic and varies based on individual preferences and opinions.\n",
      "cosine_similarity : 0.7451253144966979\n",
      "\n",
      "\n",
      "   query : Who do you think is the best soccer player and why\n",
      "document : It is difficult to pinpoint one individual as the best soccer player as it is subjective to personal preference and opinions, but players such as Lionel Messi and Cristiano Ronaldo have consistently showcased incredible skills, talent and tremendous success throughout their careers.\n",
      "cosine_similarity : 0.6982998247370353\n",
      "\n",
      "\n",
      "   query : Who do you think is the best soccer player and why\n",
      "document : I cannot provide a definitive answer to who the best soccer player is because it is subjective and dependent on individual opinions and criteria.\n",
      "cosine_similarity : 0.7431996705912162\n",
      "\n",
      "\n",
      "The most similar document to the query is:\n",
      "   query : Who do you think is the best soccer player and why\n",
      "document : I cannot definitively name the best soccer player as it is a subjective topic and varies based on individual preferences and opinions.\n"
     ]
    }
   ],
   "source": [
    "# GPT-3의 응답을 얻습니다.\n",
    "gpt_responses = talk_with(\n",
    "    persona=\"\"\"You are the best sports reporter, the audience will ask you to evaluate a soccer player. You have to be logical. You have to answer in one sentence\"\"\",\n",
    "    ask_user=\"Who do you think is the best soccer player and why\"\n",
    ")\n",
    "\n",
    "# 문서들을 불러옵니다.\n",
    "documents = gpt_responses\n",
    "\n",
    "# 문서들에 대한 임베딩을 생성합니다.\n",
    "doc_embeddings = []\n",
    "for doc in documents:\n",
    "    response = openai.Embedding.create(\n",
    "        input=doc,\n",
    "        engine=\"text-similarity-davinci-001\"\n",
    "    )\n",
    "    doc_embeddings.append(response[\"data\"][0][\"embedding\"])\n",
    "\n",
    "# 쿼리에 대한 임베딩을 생성합니다.\n",
    "query = \"Who do you think is the best soccer player and why\"\n",
    "response = openai.Embedding.create(\n",
    "    input=query,\n",
    "    engine=\"text-similarity-davinci-001\"\n",
    ")\n",
    "query_embedding = response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# 문서들과 쿼리 사이의 코사인 유사도를 계산하고 출력합니다.\n",
    "cosine_similarities = []\n",
    "for i, doc_embedding in enumerate(doc_embeddings):\n",
    "    cosine_similarity = np.dot(query_embedding, doc_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding))\n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    # 각 문서와 쿼리 사이의 코사인 유사도를 출력합니다.\n",
    "    print('   query : ' + query)\n",
    "    print('document : ' + documents[i])\n",
    "    print('cosine_similarity : ' + str(cosine_similarity))\n",
    "    print('\\n')\n",
    "\n",
    "# 가장 유사한 문서를 찾습니다.\n",
    "max_index = np.argmax(cosine_similarities)\n",
    "most_similar_document = documents[max_index]\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "print(\"The most similar document to the query is:\")\n",
    "print('   query : ' + query)\n",
    "print('document : ' + most_similar_document)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
